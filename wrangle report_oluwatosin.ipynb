{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba2353d",
   "metadata": {},
   "source": [
    "# Twitter archive wrangle report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a5c3c",
   "metadata": {},
   "source": [
    "In this report, I describe my wrangling approach for gathering and cleaning the data needed for WeRateDogs Twitter Archive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c8996",
   "metadata": {},
   "source": [
    "\n",
    "### Gathering Data for this Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd946675",
   "metadata": {},
   "source": [
    "This project required data gathering from three different sources, which are stated below. A distinct technique of data collection was utilized for each data source, which was,\n",
    "\n",
    "- I manually downloaded the file twitter archive enhanced.csv. After downloading it, I uploaded it to Pandas and read the data into a DataFrame\n",
    "\n",
    "- I downloaded the file (image predictions.tsv), which contains neural network predictions in each tweet. It is housed on the servers of Udacity, and I downloaded it programmatically using the Requests library.\n",
    "\n",
    "- Additional Data via the Twitter API: Returning to the Twitter archives, retweet count and favorite count are two notable column exclusions. Using the tweet IDs from the WeRateDogs Twitter archive, I'd use Python's Tweepy package to query the Twitter API for each tweet's JSON data, then save each tweet's whole set of JSON data in a file named tweet_json.txt. The .txt file is read into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10881f8b",
   "metadata": {},
   "source": [
    "### Assessment & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db2b67",
   "metadata": {},
   "source": [
    "\n",
    "-  Inspecting the data visually, I can see that: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id retweeted_status_user_id retweeted_status_timestamp mostly contained null datasets, all these were dropped from the dataset\n",
    "\n",
    "- Several rows of tail() output had erroneous strings in the name column, such as \"a,\" \"an,\" and \"in,\" which are the third and fourth words in the text column. There are also \"None\" options in the name column. I cleaned up the data by changing anything that wasn't a name to \"no name\" to show that there was no name.\n",
    "\n",
    "- doggo, floofer, pupper, puppo had many values represented as \"none\" was still left as none and the four columns were combined into one called dog_stage\n",
    "\n",
    "- The expanded_urls (links to images) had 59 tweets with missing data, this was eventually dropped from the dataset and the column was dropped\n",
    "\n",
    "- timestamp and retweeted_status_timestamp are object, they had wrong data types, the was corrected by converting to the right datatypes\n",
    "\n",
    "- Wrong entry of the number/number expression was recorded as the rating, Some other ratings had large rating denominators or numerators. Also some rating denominators and numerator were represented with the date/time. Removed any entries in rating denominator that are less than 10 and any rows in rating numerator that are larger than 20. Several tweets' ratings were manually adjusted using text values.) \n",
    "\n",
    "- There are only four types of values in the source column that may be obtained by modification. I used regex to obtain the necessary source from the html code\n",
    "\n",
    "- 281 missing data between the twitter_archives and image_prediction were not used\n",
    "\n",
    "- The column date_time for the api_data does not have the right datatype, this was corrected to the right datatype\n",
    "\n",
    "   \n",
    "-  I don't think there's need for the jpg url; this was dropped from the dataset\n",
    "\n",
    "-  Using an inner join based on the tweet id, merge all datasets into one. The best breed prediction and related confidence level were obtained and merged. The retweet count and favorite count columns were merged to the archive table, which was then saved to the new \"twitter archive master.csv\" file.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1f9ff",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bdb68",
   "metadata": {},
   "source": [
    "This was a really comprehensive procedure that took a lot of time and energy from me; I am glad I was able to finish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56787384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
